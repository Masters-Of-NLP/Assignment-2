{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log, exp\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from n_gram_updated import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing and Processing the Test and Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "263000"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filepath = \"train_data.csv\"\n",
    "df_train = pd.read_csv(filepath)\n",
    "df_train.rename(columns = {df_train.columns[0]:\"Value\"},inplace=True)\n",
    "df_train.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65750"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filepath = \"test_data.csv\"\n",
    "df_test = pd.read_csv(filepath)\n",
    "df_test.rename(columns = {df_test.columns[0]:\"Value\"},inplace=True)\n",
    "df_test.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Corpus\n",
    "train_corpus = {}\n",
    "sentence_id = 0\n",
    "for sentence in df_train['Value']:\n",
    "    tokenized_sentence = []\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    tokens = (token for token in tokens if token.isalpha())\n",
    "    tokenized_sentence += tokens\n",
    "    train_corpus[sentence_id] = tokenized_sentence\n",
    "    sentence_id += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Corpus\n",
    "test_corpus = {} \n",
    "sentence_id = 0\n",
    "for sentence in df_test['Value']:\n",
    "    tokenized_sentence = []\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    tokens = (token for token in tokens if token.isalpha())\n",
    "    tokenized_sentence += tokens\n",
    "    test_corpus[sentence_id] = tokenized_sentence\n",
    "    sentence_id += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66112\n"
     ]
    }
   ],
   "source": [
    "# Defining Vocabulary\n",
    "\n",
    "Vocab = vocabulary(train_corpus)\n",
    "V = len(Vocab)\n",
    "print(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count_dict = count_n_gram(train_corpus,2)\n",
    "# Nc_ = Nc(count_dict)\n",
    "# print(Nc_[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# def interpolate_with_numpy(lst):\n",
    "#     # Convert list to numpy array for convenience\n",
    "#     arr = np.array(lst)\n",
    "    \n",
    "#     # Find indices of non-zero values\n",
    "#     non_zero_indices = np.nonzero(arr)[0]\n",
    "    \n",
    "#     # Handle case where the list starts with zeros\n",
    "#     if non_zero_indices[0] != 0:\n",
    "#         non_zero_indices = np.insert(non_zero_indices, 0, 0)\n",
    "    \n",
    "#     # Interpolate zeros (excluding the last value)\n",
    "#     arr[:-1] = np.interp(np.arange(len(arr)-1), non_zero_indices, arr[non_zero_indices])\n",
    "    \n",
    "#     # Extrapolate the last value\n",
    "#     last_non_zero = arr[-2]  # As per your condition the second last value is always non-zero\n",
    "#     second_last_non_zero = arr[non_zero_indices[-2]]\n",
    "#     gradient = last_non_zero - second_last_non_zero\n",
    "#     arr[-1] = last_non_zero + gradient\n",
    "#     # arr = list(arr)\n",
    "#     return arr.tolist()\n",
    "\n",
    "# # Example\n",
    "# lst = [0, 0, 2, 0, 4, 6, 0]\n",
    "# print(interpolate_with_numpy(lst))\n",
    "\n",
    "# lst = Nc_\n",
    "# lst1 = interpolate_with_numpy(lst)\n",
    "# lst2 = remove_null(Nc_)\n",
    "# print(Nc_)\n",
    "# # print(interpolate_with_numpy(lst))\n",
    "# print(lst1)\n",
    "# print(lst2)\n",
    "# print(lst1==lst2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unigram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity for Unigram model without smoothing: 255029160.07513666\n"
     ]
    }
   ],
   "source": [
    "prob_unigram = train_n_gram(train_corpus, 1)\n",
    "pp, pp_unigram = test_n_gram(test_data=test_corpus,n=1, prob_words=prob_unigram)\n",
    "print(\"Perplexity for Unigram model without smoothing:\", pp_unigram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unigram Model with Laplace Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity for Unigram model with Laplace smoothing: 1270.0001626416067\n"
     ]
    }
   ],
   "source": [
    "prob_unigram_sm = train_n_gram(train_corpus, 1, vocab = V, smoothing=True,how='Laplace')\n",
    "pp, pp_unigram_sm = test_n_gram(test_data=test_corpus,n=1, prob_words=prob_unigram_sm, Vocabulary=V, smoothing=True, how='Laplace', processed_corpus=train_corpus)\n",
    "print(\"Perplexity for Unigram model with Laplace smoothing:\", pp_unigram_sm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unigram Model with Add-k Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity for Unigram model with Add-k smoothing: 1688.9715811695312 0.1\n",
      "Perplexity for Unigram model with Add-k smoothing: 1519.5908690793046 0.2\n",
      "Perplexity for Unigram model with Add-k smoothing: 1442.7910037763445 0.3\n",
      "Perplexity for Unigram model with Add-k smoothing: 1395.2464936658516 0.4\n",
      "Perplexity for Unigram model with Add-k smoothing: 1361.5085128295998 0.5\n",
      "Perplexity for Unigram model with Add-k smoothing: 1335.6600505660454 0.6\n",
      "Perplexity for Unigram model with Add-k smoothing: 1314.8652195956113 0.7\n",
      "Perplexity for Unigram model with Add-k smoothing: 1297.5637837565068 0.8\n",
      "Perplexity for Unigram model with Add-k smoothing: 1282.812422347832 0.9\n",
      "Perplexity for Unigram model with Add-k smoothing: 1270.0001626416067 1.0\n",
      "Perplexity for Unigram model with Add-k smoothing: 1194.1445563314903 2\n",
      "Perplexity for Unigram model with Add-k smoothing: 1156.8848198783232 3\n",
      "Perplexity for Unigram model with Add-k smoothing: 1134.3689566358137 4\n",
      "Perplexity for Unigram model with Add-k smoothing: 1119.6712529302079 5\n",
      "Perplexity for Unigram model with Add-k smoothing: 1109.8076497620898 6\n",
      "Perplexity for Unigram model with Add-k smoothing: 1103.2171528528431 7\n",
      "Perplexity for Unigram model with Add-k smoothing: 1098.980645892976 8\n",
      "Perplexity for Unigram model with Add-k smoothing: 1096.5116517305737 9\n",
      "Perplexity for Unigram model with Add-k smoothing: 1095.413430877861 10\n",
      "Perplexity for Unigram model with Add-k smoothing: 1122.0500333889356 20\n",
      "Perplexity for Unigram model with Add-k smoothing: 1175.3858769102762 30\n",
      "Perplexity for Unigram model with Add-k smoothing: 1237.1866429719291 40\n",
      "Perplexity for Unigram model with Add-k smoothing: 1302.296267027024 50\n",
      "Perplexity for Unigram model with Add-k smoothing: 1368.6644714507886 60\n",
      "Perplexity for Unigram model with Add-k smoothing: 1435.3295284496562 70\n",
      "Perplexity for Unigram model with Add-k smoothing: 1501.7964836611513 80\n",
      "Perplexity for Unigram model with Add-k smoothing: 1567.7972808270815 90\n",
      "Perplexity for Unigram model with Add-k smoothing: 1633.1839305677547 100\n",
      "Perplexity for Unigram model with Add-k smoothing: 1697.8759022656973 110\n",
      "Perplexity for Unigram model with Add-k smoothing: 1761.8322227240526 120\n"
     ]
    }
   ],
   "source": [
    "K_uni = 0\n",
    "for i in range(30):\n",
    "    if K_uni < 1:\n",
    "        K_uni += 0.1\n",
    "        K_uni = round(K_uni, 1)\n",
    "    elif K_uni < 10:\n",
    "        K_uni += 1\n",
    "        K_uni = int(K_uni)\n",
    "    else:\n",
    "        K_uni += 10\n",
    "    prob_unigram_sm_addk = train_n_gram(train_corpus, 1, vocab = V, smoothing=True,how='Add_k',k=K_uni)\n",
    "    pp, pp_unigram_sm_addk = test_n_gram(test_data=test_corpus,n=1, prob_words=prob_unigram_sm_addk, Vocabulary=V, smoothing=True, how='Add_k', k=K_uni, processed_corpus=train_corpus)\n",
    "    print(\"Perplexity for Unigram model with Add-k smoothing:\", pp_unigram_sm_addk, K_uni)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unigram Model with Good Turing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity for Unigram model with Good Turing smoothing: 968.5055318410206\n"
     ]
    }
   ],
   "source": [
    "prob_unigram_GT = train_n_gram(train_corpus, n=1, how='Good Turing')\n",
    "pp, pp_unigram_GT = test_n_gram(test_data=test_corpus, n=1, prob_words=prob_unigram_GT, how='Good Turing')\n",
    "print(\"Perplexity for Unigram model with Good Turing smoothing:\", pp_unigram_GT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bigram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity for Bigram model without smoothing: 31069869118.23457\n"
     ]
    }
   ],
   "source": [
    "prob_bigram = train_n_gram(train_corpus, 2)\n",
    "pp, pp_bigram = test_n_gram(test_data=test_corpus,n=2, prob_words=prob_bigram)\n",
    "print(\"Perplexity for Bigram model without smoothing:\", pp_bigram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bigram Model with Laplace Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity for Bigram model with Laplace smoothing: 1813.9033369573867\n"
     ]
    }
   ],
   "source": [
    "prob_bigram_sm = train_n_gram(train_corpus, 2, vocab = V, smoothing=True, how='Laplace')\n",
    "pp, pp_bigram_sm = test_n_gram(test_data=test_corpus,n=2, prob_words=prob_bigram_sm, Vocabulary=V, smoothing=True, how='Laplace', processed_corpus=train_corpus)\n",
    "print(\"Perplexity for Bigram model with Laplace smoothing:\", pp_bigram_sm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bigram Model with Add-k Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity for Bigram model with Add-k smoothing: 524.2167864319489\n"
     ]
    }
   ],
   "source": [
    "prob_bigram_sm_addk = train_n_gram(train_corpus, 2, vocab = V, smoothing=True,how='Add_k',k=0.01)\n",
    "pp, pp_bigram_sm_addk = test_n_gram(test_data=test_corpus,n=2, prob_words=prob_bigram_sm_addk, Vocabulary=V, smoothing=True, how='Add_k', k=0.01, processed_corpus=train_corpus)\n",
    "print(\"Perplexity for Bigram model with Add-k smoothing:\", pp_bigram_sm_addk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bigram Model with Good Turing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity for Bigram model with Good Turing smoothing: 26041.65749218391\n"
     ]
    }
   ],
   "source": [
    "prob_bigram_GT = train_n_gram(train_corpus, n=2, how='Good Turing')\n",
    "pp, pp_bigram_GT = test_n_gram(test_data=test_corpus, n=2, prob_words=prob_bigram_GT, how='Good Turing')\n",
    "print(\"Perplexity for Bigram model with Good Turing smoothing:\", pp_bigram_GT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trigram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity for Trigram model without smoothing: 69139995101.51898\n"
     ]
    }
   ],
   "source": [
    "prob_trigram = train_n_gram(train_corpus, 3)\n",
    "pp, pp_trigram = test_n_gram(test_data=test_corpus,n=3, prob_words=prob_trigram)\n",
    "print(\"Perplexity for Trigram model without smoothing:\", pp_trigram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trigram Model with Laplace Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity for Trigram model with Laplace smoothing: 4466.5111161327695\n"
     ]
    }
   ],
   "source": [
    "prob_trigram_sm = train_n_gram(train_corpus, 3, vocab = V, smoothing=True, how='Laplace')\n",
    "pp, pp_trigram_sm = test_n_gram(test_data=test_corpus,n=3, prob_words=prob_trigram_sm, Vocabulary=V, smoothing=True, how='Laplace', processed_corpus=train_corpus)\n",
    "print(\"Perplexity for Trigram model with Laplace smoothing:\", pp_trigram_sm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trigram Model with Add-k Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity for Trigram model with Add-k smoothing: 1301.4960365315192\n"
     ]
    }
   ],
   "source": [
    "prob_trigram_sm_addk = train_n_gram(train_corpus, 3, vocab = V, smoothing=True,how='Add_k',k=0.01)\n",
    "pp, pp_trigram_sm_addk = test_n_gram(test_data=test_corpus,n=3, prob_words=prob_trigram_sm_addk, Vocabulary=V, smoothing=True, how='Add_k', k=0.01, processed_corpus=train_corpus)\n",
    "print(\"Perplexity for Trigram model with Add-k smoothing:\", pp_trigram_sm_addk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trigram Model with Good Turing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity for Trigram model with Good Turing smoothing: 63173.0338193803\n"
     ]
    }
   ],
   "source": [
    "prob_trigram_GT = train_n_gram(train_corpus, n=3, how='Good Turing')\n",
    "pp, pp_trigram_GT = test_n_gram(test_data=test_corpus, n=3, prob_words=prob_trigram_GT, how='Good Turing')\n",
    "print(\"Perplexity for Trigram model with Good Turing smoothing:\", pp_trigram_GT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quadgram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity for Quadgram model without smoothing: 292224555035.2305\n"
     ]
    }
   ],
   "source": [
    "prob_quadgram = train_n_gram(train_corpus, 4)\n",
    "pp, pp_quadgram = test_n_gram(test_data=test_corpus,n=4, prob_words=prob_quadgram)\n",
    "print(\"Perplexity for Quadgram model without smoothing:\", pp_quadgram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quadgram Model with Laplace Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity for Quadgram model with Laplace smoothing: 5013.931269898044\n"
     ]
    }
   ],
   "source": [
    "prob_quadgram_sm = train_n_gram(train_corpus, 4, vocab = V, smoothing=True, how='Laplace')\n",
    "pp, pp_quadgram_sm = test_n_gram(test_data=test_corpus,n=4, prob_words=prob_quadgram_sm, Vocabulary=V, smoothing=True, how='Laplace', processed_corpus=train_corpus)\n",
    "print(\"Perplexity for Quadgram model with Laplace smoothing:\", pp_quadgram_sm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quadgram Model with Add-k Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity for Quadgram model with Add-k smoothing: 2362.410302953623\n"
     ]
    }
   ],
   "source": [
    "prob_quadgram_sm_addk = train_n_gram(train_corpus, 4, vocab = V, smoothing=True,how='Add_k',k=0.01)\n",
    "pp, pp_quadgram_sm_addk = test_n_gram(test_data=test_corpus,n=4, prob_words=prob_quadgram_sm_addk, Vocabulary=V, smoothing=True, how='Add_k', k=0.01, processed_corpus=train_corpus)\n",
    "print(\"Perplexity for Quadgram model with Add-k smoothing:\", pp_quadgram_sm_addk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quadgram Model with Good Turing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity for Quadgram model with Good Turing smoothing: 87033.95812694842\n"
     ]
    }
   ],
   "source": [
    "prob_quadgram_GT = train_n_gram(train_corpus, n=4, how='Good Turing')\n",
    "pp, pp_quadgram_GT = test_n_gram(test_data=test_corpus, n=4, prob_words=prob_quadgram_GT, how='Good Turing')\n",
    "print(\"Perplexity for Quadgram model with Good Turing smoothing:\", pp_quadgram_GT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perplexities of different Models on training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity for Unigram model on training data: 1236.976795496186\n",
      "Perplexity for Bigram model on training data: 76.80480458054907\n",
      "Perplexity for Trigram model on training data: 8.646154355512213\n",
      "Perplexity for Quadgram model on training data: 2.9718372121761303\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pp, pp_unigram_train = test_n_gram(test_data=train_corpus,n=1, prob_words=prob_unigram)\n",
    "print(\"Perplexity for Unigram model on training data:\", pp_unigram_train)\n",
    "\n",
    "pp, pp_bigram_train = test_n_gram(test_data=train_corpus,n=2, prob_words=prob_bigram)\n",
    "print(\"Perplexity for Bigram model on training data:\", pp_bigram_train)\n",
    "\n",
    "pp, pp_trigram_train = test_n_gram(test_data=train_corpus,n=3, prob_words=prob_trigram)\n",
    "print(\"Perplexity for Trigram model on training data:\", pp_trigram_train)\n",
    "\n",
    "pp, pp_quadgram_train = test_n_gram(test_data=train_corpus,n=4, prob_words=prob_quadgram)\n",
    "print(\"Perplexity for Quadgram model on training data:\", pp_quadgram_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
