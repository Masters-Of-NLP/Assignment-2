{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log, exp\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from n_gram import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing and Processing the Test and Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "263000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filepath = \"Dataset/Training/train_data.csv\"\n",
    "df_train = pd.read_csv(filepath)\n",
    "df_train.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65750"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filepath = \"Dataset/Testing/test_data.csv\"\n",
    "df_test = pd.read_csv(filepath)\n",
    "df_test.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Corpus\n",
    "train_corpus = {}\n",
    "sentence_id = 0\n",
    "for sentence in df_train['Value']:\n",
    "    tokenized_sentence = []\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    tokens = (token for token in tokens if token.isalpha())\n",
    "    tokenized_sentence += tokens\n",
    "    train_corpus[sentence_id] = tokenized_sentence\n",
    "    sentence_id += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Corpus\n",
    "test_corpus = {} \n",
    "sentence_id = 0\n",
    "for sentence in df_test['Value']:\n",
    "    tokenized_sentence = []\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    tokens = (token for token in tokens if token.isalpha())\n",
    "    tokenized_sentence += tokens\n",
    "    test_corpus[sentence_id] = tokenized_sentence\n",
    "    sentence_id += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72977\n"
     ]
    }
   ],
   "source": [
    "# Defining Vocabulary\n",
    "\n",
    "Vocab = vocabulary(train_corpus,test_corpus)\n",
    "V = len(Vocab)\n",
    "print(V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unigram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity for Unigram model without smoothing: 10981259472.415056\n"
     ]
    }
   ],
   "source": [
    "prob_unigram = train_n_gram(train_corpus, 1)\n",
    "pp, pp_unigram = test_n_gram(test_data=test_corpus,n=1, prob_words=prob_unigram)\n",
    "print(\"Perplexity for Unigram model without smoothing:\", pp_unigram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unigram Model with Laplace Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity for Unigram model with Laplace smoothing: 1288.081447441166\n"
     ]
    }
   ],
   "source": [
    "prob_unigram_sm = train_n_gram(train_corpus, 1, vocab = V, smoothing=True,how='Laplace')\n",
    "pp, pp_unigram_sm = test_n_gram(test_data=test_corpus,n=1, prob_words=prob_unigram_sm, Vocabulary=V, smoothing=True, how='Laplace', processed_corpus=train_corpus)\n",
    "print(\"Perplexity for Unigram model with Laplace smoothing:\", pp_unigram_sm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unigram Model with Add-k Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity for Unigram model with Add-k smoothing: 1090.37671485695 at k = 9.61\n"
     ]
    }
   ],
   "source": [
    "# Optimized k from k_optimization notebook\n",
    "prob_unigram_sm_addk = train_n_gram(train_corpus, 1, vocab = V, smoothing=True,how='Add_k',k=9.61)\n",
    "pp, pp_unigram_sm_addk = test_n_gram(test_data=test_corpus,n=1, prob_words=prob_unigram_sm_addk, Vocabulary=V, smoothing=True, how='Add_k', k=9.61, processed_corpus=train_corpus)\n",
    "print(\"Perplexity for Unigram model with Add-k smoothing:\", pp_unigram_sm_addk, \"at k =\",9.61)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unigram Model with Good Turing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity for Unigram model with Good Turing smoothing: 954.394309948336\n"
     ]
    }
   ],
   "source": [
    "prob_unigram_GT = train_n_gram(train_corpus, n=1, how='Good Turing')\n",
    "pp, pp_unigram_GT = test_n_gram(test_data=test_corpus, n=1, prob_words=prob_unigram_GT, how='Good Turing')\n",
    "print(\"Perplexity for Unigram model with Good Turing smoothing:\", pp_unigram_GT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bigram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity for Bigram model without smoothing: 41282723511.56102\n"
     ]
    }
   ],
   "source": [
    "prob_bigram = train_n_gram(train_corpus, 2)\n",
    "pp, pp_bigram = test_n_gram(test_data=test_corpus,n=2, prob_words=prob_bigram)\n",
    "print(\"Perplexity for Bigram model without smoothing:\", pp_bigram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bigram Model with Laplace Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity for Bigram model with Laplace smoothing: 1926.0717416683576\n"
     ]
    }
   ],
   "source": [
    "prob_bigram_sm = train_n_gram(train_corpus, 2, vocab = V, smoothing=True, how='Laplace')\n",
    "pp, pp_bigram_sm = test_n_gram(test_data=test_corpus,n=2, prob_words=prob_bigram_sm, Vocabulary=V, smoothing=True, how='Laplace', processed_corpus=train_corpus)\n",
    "print(\"Perplexity for Bigram model with Laplace smoothing:\", pp_bigram_sm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bigram Model with Add-k Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity for Bigram model with Add-k smoothing: 480.4042686537863\n"
     ]
    }
   ],
   "source": [
    "prob_bigram_sm_addk = train_n_gram(train_corpus, 2, vocab = V, smoothing=True,how='Add_k',k=0.001)\n",
    "pp, pp_bigram_sm_addk = test_n_gram(test_data=test_corpus,n=2, prob_words=prob_bigram_sm_addk, Vocabulary=V, smoothing=True, how='Add_k', k=0.001, processed_corpus=train_corpus)\n",
    "print(\"Perplexity for Bigram model with Add-k smoothing:\", pp_bigram_sm_addk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bigram Model with Good Turing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity for Bigram model with Good Turing smoothing: 25909.976748837806\n"
     ]
    }
   ],
   "source": [
    "prob_bigram_GT = train_n_gram(train_corpus, n=2, how='Good Turing')\n",
    "pp, pp_bigram_GT = test_n_gram(test_data=test_corpus, n=2, prob_words=prob_bigram_GT, how='Good Turing')\n",
    "print(\"Perplexity for Bigram model with Good Turing smoothing:\", pp_bigram_GT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trigram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity for Trigram model without smoothing: 52285906912.19435\n"
     ]
    }
   ],
   "source": [
    "prob_trigram = train_n_gram(train_corpus, 3)\n",
    "pp, pp_trigram = test_n_gram(test_data=test_corpus,n=3, prob_words=prob_trigram)\n",
    "print(\"Perplexity for Trigram model without smoothing:\", pp_trigram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trigram Model with Laplace Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity for Trigram model with Laplace smoothing: 4859.205008548903\n"
     ]
    }
   ],
   "source": [
    "prob_trigram_sm = train_n_gram(train_corpus, 3, vocab = V, smoothing=True, how='Laplace')\n",
    "pp, pp_trigram_sm = test_n_gram(test_data=test_corpus,n=3, prob_words=prob_trigram_sm, Vocabulary=V, smoothing=True, how='Laplace', processed_corpus=train_corpus)\n",
    "print(\"Perplexity for Trigram model with Laplace smoothing:\", pp_trigram_sm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trigram Model with Add-k Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity for Trigram model with Add-k smoothing: 881.7829049234289\n"
     ]
    }
   ],
   "source": [
    "prob_trigram_sm_addk = train_n_gram(train_corpus, 3, vocab = V, smoothing=True,how='Add_k',k=2.3e-5)\n",
    "pp, pp_trigram_sm_addk = test_n_gram(test_data=test_corpus,n=3, prob_words=prob_trigram_sm_addk, Vocabulary=V, smoothing=True, how='Add_k', k=2.3e-5, processed_corpus=train_corpus)\n",
    "print(\"Perplexity for Trigram model with Add-k smoothing:\", pp_trigram_sm_addk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trigram Model with Good Turing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity for Trigram model with Good Turing smoothing: 64541.13782619178\n"
     ]
    }
   ],
   "source": [
    "prob_trigram_GT = train_n_gram(train_corpus, n=3, how='Good Turing')\n",
    "pp, pp_trigram_GT = test_n_gram(test_data=test_corpus, n=3, prob_words=prob_trigram_GT, how='Good Turing')\n",
    "print(\"Perplexity for Trigram model with Good Turing smoothing:\", pp_trigram_GT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quadgram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity for Quadgram model without smoothing: 272609968826.675\n"
     ]
    }
   ],
   "source": [
    "prob_quadgram = train_n_gram(train_corpus, 4)\n",
    "pp, pp_quadgram = test_n_gram(test_data=test_corpus,n=4, prob_words=prob_quadgram)\n",
    "print(\"Perplexity for Quadgram model without smoothing:\", pp_quadgram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quadgram Model with Laplace Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity for Quadgram model with Laplace smoothing: 5467.091269501292\n"
     ]
    }
   ],
   "source": [
    "prob_quadgram_sm = train_n_gram(train_corpus, 4, vocab = V, smoothing=True, how='Laplace')\n",
    "pp, pp_quadgram_sm = test_n_gram(test_data=test_corpus,n=4, prob_words=prob_quadgram_sm, Vocabulary=V, smoothing=True, how='Laplace', processed_corpus=train_corpus)\n",
    "print(\"Perplexity for Quadgram model with Laplace smoothing:\", pp_quadgram_sm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quadgram Model with Add-k Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity for Quadgram model with Add-k smoothing: 1753.4454231039801\n"
     ]
    }
   ],
   "source": [
    "prob_quadgram_sm_addk = train_n_gram(train_corpus, 4, vocab = V, smoothing=True,how='Add_k',k=5e-6)\n",
    "pp, pp_quadgram_sm_addk = test_n_gram(test_data=test_corpus,n=4, prob_words=prob_quadgram_sm_addk, Vocabulary=V, smoothing=True, how='Add_k', k=5e-6, processed_corpus=train_corpus)\n",
    "print(\"Perplexity for Quadgram model with Add-k smoothing:\", pp_quadgram_sm_addk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quadgram Model with Good Turing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity for Quadgram model with Good Turing smoothing: 90194.04162073249\n"
     ]
    }
   ],
   "source": [
    "prob_quadgram_GT = train_n_gram(train_corpus, n=4, how='Good Turing')\n",
    "pp, pp_quadgram_GT = test_n_gram(test_data=test_corpus, n=4, prob_words=prob_quadgram_GT, how='Good Turing')\n",
    "print(\"Perplexity for Quadgram model with Good Turing smoothing:\", pp_quadgram_GT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perplexities of different Models on training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity for Unigram model on training data: 1238.2563332047428\n",
      "Perplexity for Bigram model on training data: 76.79950776484705\n",
      "Perplexity for Trigram model on training data: 8.636673482260596\n",
      "Perplexity for Quadgram model on training data: 2.970771827821143\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pp, pp_unigram_train = test_n_gram(test_data=train_corpus,n=1, prob_words=prob_unigram)\n",
    "print(\"Perplexity for Unigram model on training data:\", pp_unigram_train)\n",
    "\n",
    "pp, pp_bigram_train = test_n_gram(test_data=train_corpus,n=2, prob_words=prob_bigram)\n",
    "print(\"Perplexity for Bigram model on training data:\", pp_bigram_train)\n",
    "\n",
    "pp, pp_trigram_train = test_n_gram(test_data=train_corpus,n=3, prob_words=prob_trigram)\n",
    "print(\"Perplexity for Trigram model on training data:\", pp_trigram_train)\n",
    "\n",
    "pp, pp_quadgram_train = test_n_gram(test_data=train_corpus,n=4, prob_words=prob_quadgram)\n",
    "print(\"Perplexity for Quadgram model on training data:\", pp_quadgram_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
