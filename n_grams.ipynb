{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "from n_grams import *\n",
    "import numpy as np\n",
    "from math import log, exp\n",
    "import pandas as pd\n",
    "import nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"Dataset/Training/train_data.csv\"\n",
    "df = pd.read_csv(filepath)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns = {df.columns[0]:\"Value\"},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_corpus = {} \n",
    "sentence_id = 0\n",
    "for sentence in df['Value']:\n",
    "    tokenized_sentence = []\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    tokens = (token for token in tokens if token.isalpha())\n",
    "    tokenized_sentence += tokens\n",
    "    processed_corpus[sentence_id] = tokenized_sentence\n",
    "    sentence_id += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(processed_corpus[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unigram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_counts = {}\n",
    "unigram_counts[\"<s>\"] = 0\n",
    "unigram_counts[\"</s>\"] = 0\n",
    "for id in processed_corpus:\n",
    "    for token in processed_corpus[id]:\n",
    "        if token not in unigram_counts:\n",
    "            unigram_counts[token] = 0\n",
    "        unigram_counts[token] += 1\n",
    "    unigram_counts[\"<s>\"] += 1\n",
    "    unigram_counts[\"</s>\"] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66112"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unigram_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"Dataset/Testing/test_data.csv\"\n",
    "df_test = pd.read_csv(filepath)\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.rename(columns = {df_test.columns[0]:\"Value\"},inplace=True)\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_corpus = {}\n",
    "test_tokens = set()\n",
    "sentence_id = 0\n",
    "num_test_words = 0\n",
    "for sentence in df_test['Value']:\n",
    "    tokenized_sentence = []\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    tokens = (token for token in tokens if token.isalpha())\n",
    "    tokenized_sentence += tokens\n",
    "    test_corpus[sentence_id] = tokenized_sentence\n",
    "    num_test_words += 2 + len(tokenized_sentence)\n",
    "    for token in tokenized_sentence:\n",
    "        test_tokens.add(token)\n",
    "    sentence_id += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_train = 0\n",
    "for token in unigram_counts:\n",
    "    N_train += unigram_counts[token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating perplexity\n",
    "\n",
    "prob_uni_words = {}\n",
    "prob_uni_words[\"<s>\"] = unigram_counts[\"<s>\"] / N_train\n",
    "prob_uni_words[\"</s>\"] = unigram_counts[\"</s>\"] / N_train\n",
    "for token in test_tokens:\n",
    "        if token in unigram_counts:\n",
    "            prob_uni_words[token] = unigram_counts[token]/N_train\n",
    "        else:\n",
    "            prob_uni_words[token] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6865"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_of_vocab = 0\n",
    "for i in prob_uni_words:\n",
    "    if(prob_uni_words[i]==0):\n",
    "        out_of_vocab+=1\n",
    "out_of_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "perplexity = {}\n",
    "ep=1e-15\n",
    "pp = 0\n",
    "# We are neglecting the words that does not appear in the train data\n",
    "for id in test_corpus:\n",
    "    N = 2 + len(test_corpus[id])\n",
    "    temp = log(prob_uni_words[\"<s>\"])\n",
    "    for token in test_corpus[id]:\n",
    "        if(prob_uni_words[token]==0):\n",
    "            temp += log(prob_uni_words[token]+ep)\n",
    "        else:\n",
    "            temp += log(prob_uni_words[token])\n",
    "    temp += log(prob_uni_words[\"</s>\"])\n",
    "    temp = (-temp)/N\n",
    "    perplexity[id] = exp(temp)\n",
    "    pp += perplexity[id]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxi = 0\n",
    "for i in range(len(perplexity)):\n",
    "\n",
    "    maxi = max(perplexity[i],maxi)\n",
    "    # print(perplexity[i])\n",
    "print(maxi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity of Unigram Model without smoothing is:  255029160.07513976\n"
     ]
    }
   ],
   "source": [
    "Unigram_pp = pp/len(test_corpus)\n",
    "print(\"Perplexity of Unigram Model without smoothing is: \", Unigram_pp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unigram Model with Laplace Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66110\n",
      "72975\n"
     ]
    }
   ],
   "source": [
    "vocab = set()\n",
    "for id in processed_corpus:\n",
    "    for token in processed_corpus[id]:\n",
    "        vocab.add(token)\n",
    "print(len(vocab))\n",
    "for id in test_corpus:\n",
    "    for token in test_corpus[id]:\n",
    "        vocab.add(token)\n",
    "print(len(vocab))\n",
    "V = len(vocab) + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_uni_sm_words = {}\n",
    "prob_uni_sm_words[\"<s>\"] = (unigram_counts[\"<s>\"] + 1) / (N_train + V)\n",
    "prob_uni_sm_words[\"</s>\"] = (unigram_counts[\"</s>\"] + 1) / (N_train + V)\n",
    "for token in test_tokens:\n",
    "        if token in unigram_counts:\n",
    "            prob_uni_sm_words[token] = (unigram_counts[token] + 1) / (N_train + V)\n",
    "        else:\n",
    "              prob_uni_sm_words[token] = 1 / (N_train + V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "perplexity_sm = {}\n",
    "pp_sm = 0\n",
    "# We are neglecting the words that does not appear in the train data\n",
    "for id in test_corpus:\n",
    "    N = 2 + len(test_corpus[id])\n",
    "    temp = log(prob_uni_sm_words[\"<s>\"])\n",
    "    for token in test_corpus[id]:\n",
    "        temp += log(prob_uni_sm_words[token])\n",
    "    temp += log(prob_uni_sm_words[\"</s>\"])\n",
    "    temp=(-temp)/N\n",
    "    perplexity_sm[id] = exp(temp)\n",
    "    pp_sm += perplexity_sm[id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity of Unigram Model with Laplace Smoothing is:  1277.93160029543\n"
     ]
    }
   ],
   "source": [
    "Unigram_sm_pp = pp_sm/len(test_corpus)\n",
    "print(\"Perplexity of Unigram Model with Laplace Smoothing is: \", Unigram_sm_pp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bigram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_counts = {}\n",
    "for id in processed_corpus:\n",
    "    tokens = processed_corpus[id]\n",
    "    size = len(tokens)\n",
    "    if((\"<s> \"+tokens[0]) not in bigram_counts):\n",
    "        bigram_counts[\"<s> \"+ tokens[0]] = 0\n",
    "    bigram_counts[\"<s> \"+tokens[0]] += 1\n",
    "    for i in range(0,size-1):\n",
    "        str = tokens[i] + \" \" + tokens[i+1]\n",
    "        if(str not in bigram_counts):\n",
    "            bigram_counts[str] = 0\n",
    "        bigram_counts[str] += 1\n",
    "    if ((tokens[size-1] + \" </s>\") not in bigram_counts):\n",
    "        bigram_counts[tokens[size-1] + \" </s>\"] = 0\n",
    "    bigram_counts[tokens[size-1] + \" </s>\"]+= 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "871874"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bigram_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_bi_words = {}\n",
    "for id in test_corpus:\n",
    "    tokens = [\"<s>\"]\n",
    "    tokens += test_corpus[id]\n",
    "    tokens.append(\"</s>\")\n",
    "    size = len(tokens)\n",
    "    for i in range(0,size-1):\n",
    "        key = tokens[i]+ \" \"+ tokens[i+1]\n",
    "        if key in bigram_counts:\n",
    "            prob_bi_words[key] = bigram_counts[key] / unigram_counts[tokens[i]]\n",
    "        else:\n",
    "            prob_bi_words[key] = 0\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_perplexity = {}\n",
    "bi_pp = 0\n",
    "ep=1e-15\n",
    "# We are neglecting the words that does not appear in the train data\n",
    "for id in test_corpus:\n",
    "    N = 2 + len(test_corpus[id])\n",
    "    tokens = [\"<s>\"]\n",
    "    tokens += test_corpus[id]\n",
    "    tokens.append(\"</s>\")\n",
    "    size = len(tokens)\n",
    "    temp = 0\n",
    "    for i in range(0,size-1):\n",
    "        str = tokens[i] + \" \" + tokens[i+1]\n",
    "        if prob_bi_words[str]!=0:\n",
    "            temp += log(prob_bi_words[str])\n",
    "        else:\n",
    "            temp += log(prob_bi_words[str]+ep)\n",
    "    temp = (-temp)/N\n",
    "    bi_perplexity[id] = exp(temp)\n",
    "    bi_pp += bi_perplexity[id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(len(bi_perplexity)):\n",
    "    print(bi_perplexity[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity of Bigram Model without smoothing is:  31069869118.23472\n"
     ]
    }
   ],
   "source": [
    "Bigram_pp = bi_pp/len(test_corpus)\n",
    "print(\"Perplexity of Bigram Model without smoothing is: \", Bigram_pp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bigram Model with Laplace Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_bi_sm_words = {}\n",
    "for key in bigram_counts:\n",
    "    w1,w2 = key.split(\" \")\n",
    "    if(key not in prob_bi_sm_words):\n",
    "        prob_bi_sm_words[key] = (bigram_counts[key] + 1) / (unigram_counts[w1] + V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_perplexity_sm = {}\n",
    "bi_pp_sm = 0\n",
    "# We are neglecting the words that does not appear in the train data\n",
    "for id in test_corpus:\n",
    "    N = 2 + len(test_corpus[id])\n",
    "    tokens = [\"<s>\"]\n",
    "    tokens += test_corpus[id]\n",
    "    tokens.append(\"</s>\")\n",
    "    size = len(tokens)\n",
    "    temp = 0\n",
    "    for i in range(0,size-1):\n",
    "        str = tokens[i] + \" \" + tokens[i+1]\n",
    "        if(str in prob_bi_sm_words):\n",
    "            temp += log(prob_bi_sm_words[str])\n",
    "        elif (tokens[i] in unigram_counts):\n",
    "            p = 1 / (unigram_counts[tokens[i]] + V)\n",
    "            temp += log(p)\n",
    "        else:\n",
    "            p = 1/V\n",
    "            temp += log(p)\n",
    "    temp = (-temp)/N\n",
    "    bi_perplexity_sm[id] = exp(temp)\n",
    "    bi_pp_sm += bi_perplexity_sm[id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity of Bigram Model with smoothing is:  1985.1815331914747\n"
     ]
    }
   ],
   "source": [
    "Bigram_sm_pp = bi_pp_sm/len(test_corpus)\n",
    "print(\"Perplexity of Bigram Model with smoothing is: \", Bigram_sm_pp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trigram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_counts = {}\n",
    "for id in processed_corpus:\n",
    "    tokens = [\"<s>\", \"<s>\"]\n",
    "    tokens += processed_corpus[id] + [\"</s>\", \"</s>\"]\n",
    "    size = len(tokens)\n",
    "    for i in range(0,size-2):\n",
    "        str = tokens[i] + \" \" + tokens[i+1] + \" \" + tokens[i+2]\n",
    "        if(str not in trigram_counts):\n",
    "            trigram_counts[str] = 0\n",
    "        trigram_counts[str] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2136765\n"
     ]
    }
   ],
   "source": [
    "print(len(trigram_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_tri_words = {}\n",
    "for key in trigram_counts:\n",
    "    w1,w2,w3 = key.split(\" \")\n",
    "    prev = w1 + \" \" + w2\n",
    "    if (prev in bigram_counts):\n",
    "        prob_tri_words[key] = trigram_counts[key] / bigram_counts[prev]\n",
    "    else:\n",
    "        prob_tri_words[key] = trigram_counts[key] / unigram_counts[w2]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "tri_perplexity = {}\n",
    "tri_pp = 0\n",
    "ep=1e-15\n",
    "# We are neglecting the words that does not appear in the train data\n",
    "for id in test_corpus:\n",
    "    N = 4 + len(test_corpus[id])\n",
    "    tokens = [\"<s>\", \"<s>\"]\n",
    "    tokens += test_corpus[id] + [\"</s>\", \"</s>\"]\n",
    "    size = len(tokens)\n",
    "    temp = 0\n",
    "    for i in range(0,size-2):\n",
    "        str = tokens[i] + \" \" + tokens[i+1] + \" \" + tokens[i+2]\n",
    "        if(str in prob_tri_words):\n",
    "            temp += log(prob_tri_words[str])\n",
    "        else:\n",
    "            temp +=log(ep)\n",
    "    temp = (-temp)/N\n",
    "    tri_perplexity[id] = exp(temp)\n",
    "    tri_pp += tri_perplexity[id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity of Trigram Model without smoothing is:  69139995101.5171\n"
     ]
    }
   ],
   "source": [
    "Trigram_pp = tri_pp/len(test_corpus)\n",
    "print(\"Perplexity of Trigram Model without smoothing is: \", Trigram_pp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trigram Model with Laplace Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_tri_sm_words = {}\n",
    "for key in trigram_counts:\n",
    "    w1,w2,w3 = key.split(\" \")\n",
    "    prev = w1 + \" \" + w2\n",
    "    if (prev in bigram_counts):\n",
    "        prob_tri_words[key] = (trigram_counts[key] + 1) / (bigram_counts[prev] + V)\n",
    "    else:\n",
    "        prob_tri_words[key] = (trigram_counts[key] + 1) / (unigram_counts[w2] + V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "tri_perplexity_sm = {}\n",
    "tri_pp_sm = 0\n",
    "# We are neglecting the words that does not appear in the train data\n",
    "for id in test_corpus:\n",
    "    N = 4 + len(test_corpus[id])\n",
    "    tokens = [\"<s>\", \"<s>\"]\n",
    "    tokens += test_corpus[id] + [\"</s>\", \"</s>\"]\n",
    "    size = len(tokens)\n",
    "    temp = 0\n",
    "    for i in range(0,size-2):\n",
    "        str = tokens[i] + \" \" + tokens[i+1] + \" \" + tokens[i+2]\n",
    "        if(str in prob_tri_sm_words):\n",
    "            temp += log(prob_tri_sm_words[str])\n",
    "        else:\n",
    "            prev = tokens[i] + \" \" + tokens[i+1]\n",
    "            if (prev in bigram_counts):\n",
    "                p = 1 /(bigram_counts[prev] + V)\n",
    "                temp += log(p)\n",
    "            elif (tokens[i+1] in unigram_counts):\n",
    "                p = 1 /(unigram_counts[tokens[i+1]] + V)\n",
    "                temp += log(p)\n",
    "            else:\n",
    "                p = 1/V\n",
    "                temp += log(p)\n",
    "    temp = (-temp)/N\n",
    "    tri_perplexity_sm[id] = exp(temp)\n",
    "    tri_pp_sm += tri_perplexity_sm[id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity of Trigram Model without smoothing is:  20458.98937238894\n"
     ]
    }
   ],
   "source": [
    "Trigram_sm_pp = tri_pp_sm/len(test_corpus)\n",
    "print(\"Perplexity of Trigram Model without smoothing is: \", Trigram_sm_pp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
