{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log, exp\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from n_gram import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing and Processing the Test and Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "263000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filepath = \"Dataset/Training/train_data.csv\"\n",
    "df_train = pd.read_csv(filepath)\n",
    "df_train.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65750"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filepath = \"Dataset/Testing/test_data.csv\"\n",
    "df_test = pd.read_csv(filepath)\n",
    "df_test.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Corpus\n",
    "train_corpus = {} \n",
    "sentence_id = 0\n",
    "for sentence in df_train['Value']:\n",
    "    tokenized_sentence = []\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    tokens = (token for token in tokens if token.isalpha())\n",
    "    tokenized_sentence += tokens\n",
    "    train_corpus[sentence_id] = tokenized_sentence\n",
    "    sentence_id += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Corpus\n",
    "test_corpus = {} \n",
    "sentence_id = 0\n",
    "for sentence in df_test['Value']:\n",
    "    tokenized_sentence = []\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    tokens = (token for token in tokens if token.isalpha())\n",
    "    tokenized_sentence += tokens\n",
    "    test_corpus[sentence_id] = tokenized_sentence\n",
    "    sentence_id += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Vocabulary\n",
    "\n",
    "Vocab = vocabulary(train_corpus, test_corpus)\n",
    "V = len(Vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unigram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity for Unigram model without smoothing: 10981259472.415056\n"
     ]
    }
   ],
   "source": [
    "prob_unigram = train_n_gram(train_corpus, 1)\n",
    "pp, pp_unigram = test_n_gram(test_data=test_corpus,n=1, prob_words=prob_unigram)\n",
    "print(\"Perplexity for Unigram model without smoothing:\", pp_unigram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unigram Model with Laplace Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity for Unigram model with Laplace smoothing: 850.6429063343785\n"
     ]
    }
   ],
   "source": [
    "prob_unigram_sm = train_n_gram(train_corpus, 1, vocab = V, smoothing=True,how='Laplace')\n",
    "pp, pp_unigram_sm = test_n_gram(test_data=test_corpus,n=1, prob_words=prob_unigram_sm, Vocabulary=V, smoothing=True, how='Laplace', processed_corpus=train_corpus)\n",
    "print(\"Perplexity for Unigram model with Laplace smoothing:\", pp_unigram_sm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unigram Model with Add-k Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity for Unigram model with Add-k smoothing: 7686.053675955733\n"
     ]
    }
   ],
   "source": [
    "prob_unigram_sm_addk = train_n_gram(train_corpus, 1, vocab = V, smoothing=True,how='Add_k',k=0.01)\n",
    "pp, pp_unigram_sm_addk = test_n_gram(test_data=test_corpus,n=1, prob_words=prob_unigram_sm_addk, Vocabulary=V, smoothing=True, how='Add_k', k=0.01, processed_corpus=train_corpus)\n",
    "print(\"Perplexity for Unigram model with Add-k smoothing:\", pp_unigram_sm_addk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bigram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity for Bigram model without smoothing: 41282723511.56102\n"
     ]
    }
   ],
   "source": [
    "prob_bigram = train_n_gram(train_corpus, 2)\n",
    "pp, pp_bigram = test_n_gram(test_data=test_corpus,n=2, prob_words=prob_bigram)\n",
    "print(\"Perplexity for Bigram model without smoothing:\", pp_bigram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bigram Model with Laplace Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity for Bigram model with Laplace smoothing: 1795.1983453812102\n"
     ]
    }
   ],
   "source": [
    "prob_bigram_sm = train_n_gram(train_corpus, 2, vocab = V, smoothing=True, how='Laplace')\n",
    "pp, pp_bigram_sm = test_n_gram(test_data=test_corpus,n=2, prob_words=prob_bigram_sm, Vocabulary=V, smoothing=True, how='Laplace', processed_corpus=train_corpus)\n",
    "print(\"Perplexity for Bigram model with Laplace smoothing:\", pp_bigram_sm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bigram Model with Add-k Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity for Bigram model with Add-k smoothing: 539.7113764829446\n"
     ]
    }
   ],
   "source": [
    "prob_bigram_sm_addk = train_n_gram(train_corpus, 2, vocab = V, smoothing=True,how='Add_k',k=0.01)\n",
    "pp, pp_bigram_sm_addk = test_n_gram(test_data=test_corpus,n=2, prob_words=prob_bigram_sm_addk, Vocabulary=V, smoothing=True, how='Add_k', k=0.01, processed_corpus=train_corpus)\n",
    "print(\"Perplexity for Bigram model with Add-k smoothing:\", pp_bigram_sm_addk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trigram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity for Trigram model without smoothing: 52285906912.19435\n"
     ]
    }
   ],
   "source": [
    "prob_trigram = train_n_gram(train_corpus, 3)\n",
    "pp, pp_trigram = test_n_gram(test_data=test_corpus,n=3, prob_words=prob_trigram)\n",
    "print(\"Perplexity for Trigram model without smoothing:\", pp_trigram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trigram Model with Laplace Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity for Trigram model with Laplace smoothing: 4718.023443377587\n"
     ]
    }
   ],
   "source": [
    "prob_trigram_sm = train_n_gram(train_corpus, 3, vocab = V, smoothing=True, how='Laplace')\n",
    "pp, pp_trigram_sm = test_n_gram(test_data=test_corpus,n=3, prob_words=prob_trigram_sm, Vocabulary=V, smoothing=True, how='Laplace', processed_corpus=train_corpus)\n",
    "print(\"Perplexity for Trigram model with Laplace smoothing:\", pp_trigram_sm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trigram Model with Add-k Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity for Trigram model with Add-k smoothing: 1391.2340984101183\n"
     ]
    }
   ],
   "source": [
    "prob_trigram_sm_addk = train_n_gram(train_corpus, 3, vocab = V, smoothing=True,how='Add_k',k=0.01)\n",
    "pp, pp_trigram_sm_addk = test_n_gram(test_data=test_corpus,n=3, prob_words=prob_trigram_sm_addk, Vocabulary=V, smoothing=True, how='Add_k', k=0.01, processed_corpus=train_corpus)\n",
    "print(\"Perplexity for Trigram model with Add-k smoothing:\", pp_trigram_sm_addk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quadgram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity for Quadgram model without smoothing: 272609968826.675\n"
     ]
    }
   ],
   "source": [
    "prob_quadgram = train_n_gram(train_corpus, 4)\n",
    "pp, pp_quadgram = test_n_gram(test_data=test_corpus,n=4, prob_words=prob_quadgram)\n",
    "print(\"Perplexity for Quadgram model without smoothing:\", pp_quadgram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quadgram Model with Laplace Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity for Quadgram model with Laplace smoothing: 5352.454740782293\n"
     ]
    }
   ],
   "source": [
    "prob_quadgram_sm = train_n_gram(train_corpus, 4, vocab = V, smoothing=True, how='Laplace')\n",
    "pp, pp_quadgram_sm = test_n_gram(test_data=test_corpus,n=4, prob_words=prob_quadgram_sm, Vocabulary=V, smoothing=True, how='Laplace', processed_corpus=train_corpus)\n",
    "print(\"Perplexity for Quadgram model with Laplace smoothing:\", pp_quadgram_sm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quadgram Model with Add-k Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity for Quadgram model with Add-k smoothing: 2561.770566949204\n"
     ]
    }
   ],
   "source": [
    "prob_quadgram_sm_addk = train_n_gram(train_corpus, 4, vocab = V, smoothing=True,how='Add_k',k=0.01)\n",
    "pp, pp_quadgram_sm_addk = test_n_gram(test_data=test_corpus,n=4, prob_words=prob_quadgram_sm_addk, Vocabulary=V, smoothing=True, how='Add_k', k=0.01, processed_corpus=train_corpus)\n",
    "print(\"Perplexity for Quadgram model with Add-k smoothing:\", pp_quadgram_sm_addk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perplexities of different Models on training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity for Unigram model on training data: 1238.2563332047428\n",
      "Perplexity for Bigram model on training data: 76.79950776484705\n",
      "Perplexity for Trigram model on training data: 8.636673482260596\n",
      "Perplexity for Quadgram model on training data: 2.970771827821143\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pp, pp_unigram_train = test_n_gram(test_data=train_corpus,n=1, prob_words=prob_unigram)\n",
    "print(\"Perplexity for Unigram model on training data:\", pp_unigram_train)\n",
    "\n",
    "pp, pp_bigram_train = test_n_gram(test_data=train_corpus,n=2, prob_words=prob_bigram)\n",
    "print(\"Perplexity for Bigram model on training data:\", pp_bigram_train)\n",
    "\n",
    "pp, pp_trigram_train = test_n_gram(test_data=train_corpus,n=3, prob_words=prob_trigram)\n",
    "print(\"Perplexity for Trigram model on training data:\", pp_trigram_train)\n",
    "\n",
    "pp, pp_quadgram_train = test_n_gram(test_data=train_corpus,n=4, prob_words=prob_quadgram)\n",
    "print(\"Perplexity for Quadgram model on training data:\", pp_quadgram_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
